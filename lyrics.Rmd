---
title: "lyrics"
output: html_notebook
---


```{r message=FALSE, warning=FALSE, include=FALSE}
library(ggplot2)
library(tidyverse)
library(readxl)
library(reshape)
library(reshape2)
library(mongolite)
library(textcat)
library(tm)

```



```{r}
lyrics = mongo(collection = "lyrics", db = "spotify_dm" )
df_lyrics <- lyrics$find('{}')

# write.csv(df_lyrics, "data/df_lyrics.csv")


df_lyrics <- read.csv("data/df_lyrics.csv") %>% 
  select(-X)

df_lyrics_unicas <- df_lyrics %>% 
  distinct(artist_name, track_name, lyrics)


#filtro de idioma
spa_lyrics = df_lyrics_unicas[textcat(df_lyrics_unicas$lyrics)=="spanish",]
spa_lyrics

en_lyrics = df_lyrics_unicas[textcat(df_lyrics_unicas$lyrics) %in% c("english", "scots"),]
en_lyrics

#chequeo cantidad de canciones por idioma
100*(nrow(en_lyrics) + nrow(spa_lyrics))/nrow(df_lyrics_unicas)

# tabla contingencia de idiomas
idiomas = textcat(df_lyrics_unicas$lyrics)
sort(table(idiomas), decreasing = T)

```


```{r}
# comentar y descomentar según se elija un dataframe u otro
# df_lyrics_seleccionado = df_lyrics_unicas
df_lyrics_seleccionado = en_lyrics

corpus = Corpus(VectorSource(enc2utf8(df_lyrics_seleccionado$lyrics)))

inspect(corpus[1])

# Eliminamos espacios
corpus.pro <- tm_map(corpus, stripWhitespace)
# inspect(corpus.pro[1])

# Elimino todo lo que aparece antes del primer []
corpus.pro <- tm_map(corpus.pro, content_transformer(
  function(x) sub('^.+?\\[.*?\\]',"", x)))
# inspect(corpus.pro[1])

# Elimino las aclaraciones en las canciones, por ejemplo:
# [Verso 1: Luis Fonsi & Daddy Yankee]
corpus.pro <- tm_map(corpus.pro, content_transformer(
  function(x) gsub('\\[.*?\\]', '', x)))

# Elimino todo lo que aparece luego de 'More on Genius'
corpus.pro <- tm_map(corpus.pro, content_transformer(function(x) gsub("More on Genius.*","", x)))
# inspect(corpus.pro[1])

# Convertimos el texto a minúsculas
corpus.pro <- tm_map(corpus.pro, content_transformer(tolower))

# removemos números
corpus.pro <- tm_map(corpus.pro, removeNumbers)

# Podemos agregar palabras a las stopwords
# my_stopwords <- append(stopwords("spanish"), 'palabra')
my_stopwords <- append(stopwords("english"), c('yeah', "aint", "get", "got"))

# Removemos palabras vacias 
corpus.pro <- tm_map(corpus.pro, removeWords, stopwords("english"))
corpus.pro <- tm_map(corpus.pro, removeWords, my_stopwords)
# corpus.pro <- tm_map(corpus.pro, removeWords, stopwords("spanish"))
inspect(corpus.pro[1])


# Removemos puntuaciones
corpus.pro <- tm_map(corpus.pro, removePunctuation)

# Removemos todo lo que no es alfanumérico
corpus.pro <- tm_map(corpus.pro, content_transformer(function(x) str_replace_all(x, "[[:punct:]]", " ")))

# En tm_map podemos utilizar funciones prop
library(stringi)
replaceAcentos <- function(x) {stri_trans_general(x, "Latin-ASCII")}
corpus.pro <- tm_map(corpus.pro, replaceAcentos)

# Eliminamos espacios que se van generando con los reemplazos
corpus.pro <- tm_map(corpus.pro, stripWhitespace)
```


```{r}
####################################################################
####### Generación de la Matríz Término-Documento del corpus #######
####################################################################
corpus.pro2tdm <- function(corpus, ponderacion, n_terms){
  #matriz TD 
  dtm <- TermDocumentMatrix(corpus,
                            control = list(weighting = ponderacion))
  matriz_td <- as.matrix(dtm)
  
  
  # Calculamos la frecuencia de cada término en el corpus
  freq_term <- head(sort(rowSums(matriz_td),decreasing=TRUE), n_terms)
  
  #matriz transpuesta de los n_terms mas frecuentes
  matriz_nf <- t(matriz_td[sort(names(freq_term)), ])
  
  #pasaje a binario
  matriz_nf[matriz_nf>0] <- 1
  
  return(matriz_nf)
  
  }
  
matriz <- corpus.pro2tdm(corpus = corpus.pro, ponderacion= "weightTf",n_terms= 150)
dim(matriz)

df_tm <- as.data.frame(matriz)
head(df_tm,2)

## Join matriz de palabras con artista y track
df_ly_feat <- cbind(df_lyrics_seleccionado[-c(3)], df_tm)

nrow(df_tm)
nrow(df_lyrics_seleccionado)
nrow(df_ly_feat)

filter <- !names(df_ly_feat) %in% c("artist_name", "track_name" )

df_ly_feat_ok <- df_ly_feat[, filter]
# df_ly_feat_ok = df_ly_feat_ok[, -(which(colSums(df_ly_feat_ok) == 0))]

# colSums(df_ly_feat_ok)

head(df_ly_feat_ok, 3)
head(df_ly_feat, 3)
```
```{r}
df_ly_feat$id = 1:nrow(df_ly_feat)

df_melt <- reshape2::melt(data = df_ly_feat[,3:ncol(df_ly_feat)], id.vars = c("id"))  %>% arrange(id)

df_melt <- df_melt[df_melt$value != 0,]

df_melt_txt <- df_melt[df_melt$value == 1,]
df_melt_cat <- df_melt[df_melt$value != 1,]

head(df_melt_txt )
dim(df_melt_txt )

df_melt_txt$variable <- paste0("TERM_", df_melt_txt$variable)


df_melt_txt_to_ruls <- df_melt_txt[, -c(3)]
names(df_melt_txt_to_ruls) <- c("id", "item")

write.table(df_melt_txt_to_ruls, file="data/transaccions_lyrics_features.txt", row.names = F)

# Reglas
library(arules)
# chequear nan's
lyrics_trans <- read.transactions("data/transaccions_lyrics_features.txt", format = "single", cols = c(1,2))

inspect(head(lyrics_trans, 3))

summary(lyrics_trans)
reglas <- apriori(lyrics_trans, parameter = list(support=0.1,
                    confidence = 0.5, target  = "rules"  ))

reglas_sub <- subset(reglas, subset = rhs %pin% "fuck")
inspect(head(sort(reglas_sub, by = "lift", decreasing = T),5))
```




```{r}

# Dataframe con frecuencia de terminos (de rows)
dtm_to_freq <- TermDocumentMatrix(corpus.pro, control = list(weighting =  "weightTf"))
matriz_td_to_freq <- as.matrix(dtm_to_freq)
freq_term <- sort(rowSums(matriz_td_to_freq), decreasing=TRUE )
df_freq <- data.frame(termino = names(freq_term), frecuencia=freq_term)
row.names(df_freq) <- NULL
head(df_freq)


## Graficos de terminos
N=15
barplot(df_freq[1:N,]$frecuencia, las = 2, names.arg = df_freq[1:N,]$termino,
        col ="lightblue", main ="Palabras más frecuentes",
        ylab = "Frecuencia de palabras", ylim = c(0, max(df_freq$frecuencia)+300))


topK = head(df_freq, 100)

# Visualización de los resultados
# Nube de Etiquetas
library("wordcloud")
library("RColorBrewer")

par(bg="grey30") # Fijamos el fondo en color gris

set.seed(1234)
wordcloud(words = topK$termino, freq = topK$frecuencia, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(4, "Dark2"))



```


# Explicit
```{r}
library(sentimentr)
decontracted = function(txt){
  txt = gsub("won't", "will not", txt)
  txt = gsub("\\'s", " is", txt)
  txt = gsub("\\'t", " not", txt)
  txt = gsub("\\'ll", " will", txt)
  txt = gsub("\\'m", " am", txt)
  txt = gsub("\\'re", " are", txt)
  txt = gsub("\\'d", " had", txt)
  txt = gsub("\\'ve", " have", txt)
  txt = gsub("couldn", "could", txt)
  txt = gsub("don", "do", txt)
  txt = gsub("doesn", "does", txt)
  txt = gsub("isn", "is", txt)
  txt = gsub("mustn", "must", txt)
  txt = gsub("shouldn", "should", txt)
  txt = gsub("wasn", "was", txt)
  txt = gsub("\\'cause", " because", txt)
  txt = gsub("\\'", "g", txt)
  return(txt)
}


#Función para limpiar. 
text_cleaning = function(txt, stop=FALSE){
  txt = sub('^.+?\\[.*?\\]',"", txt) #ok
  txt = sub("More on Genius.*","", txt)
  txt = gsub('\\[.*?\\]', '', txt)
  txt = gsub("\\n"," ", txt)
  txt = gsub("[()]", " ", txt)
  txt = tolower(txt)
  txt = decontracted(txt)
  txt = gsub("\\W+\\b", " ", txt)
  txt = gsub("\\d", " ", txt)
  return(txt)
}

#función para obtener oraciones de una sola palabra. 
one_word_setences = function(txt){
  return(gsub("\\W+\\b", ". ", txt))
}

t = text_cleaning(en_lyrics$lyrics[1])

#Genero lista de malas palabras
bad_words <- c()
bad_words <- append(bad_words, unique(tolower(lexicon::profanity_zac_anger)))
bad_words <- append(bad_words, unique(tolower(lexicon::profanity_alvarez)))
bad_words <- append(bad_words, unique(tolower(lexicon::profanity_arr_bad)))
bad_words <- append(bad_words, unique(tolower(lexicon::profanity_racist)))
bad_words <- append(bad_words, unique(tolower(lexicon::profanity_banned)))
bad_words <- unique(bad_words)

biglou <- read.csv("https://www.cs.cmu.edu/~biglou/resources/bad-words.txt", header=FALSE, col.names = c("words"))

#Función para obtener palabras profanas de cada lyric
get_profanities = function(txt, profanity_lst){
  txt = text_cleaning(txt)
  words = as.data.frame(strsplit(txt, "[ ]+"), col.names = "words")
  profan_df = profanity(get_sentences(words), profanity_list = profanity_lst)
  profan_words = profan_df[profan_df$profanity_count!=0,]$words
  vector = as.vector(profan_words)
  if (length(vector)==0){
    return(NULL)
  }
  else{return(as.vector(profan_words))
    }
}

en_lyrics$profabe_biglou <- lapply(en_lyrics$lyrics, function(x) get_profanities(x, biglou$words))

en_lyrics$profabe_badwords <- lapply(en_lyrics$lyrics, function(x) get_profanities(x, bad_words))



```

